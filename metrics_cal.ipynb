 {
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "260e6b31-5243-44a8-a509-236261c99480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Class      Recall    Precision    AP@0.5\n",
      "\n",
      "类别    Recall   Precision  AP@0.5  \n",
      "-----------------------------------\n",
      "MTL   0.5027   0.1479     0.1914  \n",
      "TEE   0.7368   0.5185     0.6546  \n",
      "BND   0.5732   0.1957     0.2286  \n",
      "CRC   0.4181   0.0628     0.1344  \n",
      "BRN   0.5556   0.2941     0.4434  \n",
      "GWA   0.1734   0.1398     0.0502  \n",
      "SWA   0.4078   0.0842     0.0895  \n",
      "ESP   0.7778   0.3043     0.4765  \n",
      "VAL   0.8696   0.6667     0.7032  \n",
      "FLA   0.5000   0.0541     0.2794  \n",
      "CAS   0.3200   0.4444     0.2143  \n",
      "SLE   0.7931   0.2754     0.6854  \n",
      "\n",
      "result:\n",
      "========================================\n",
      "                       \n",
      "----------------------------------------\n",
      "Macro-P         0.2657\n",
      "Macro-R         0.5523\n",
      "Macro-F1        0.3323\n",
      "Micro-P         0.1382\n",
      "Micro-R         0.4601\n",
      "Micro-F1        0.2126\n",
      "mAP@0.5         0.3459\n",
      "\n",
      ":\n",
      "GT: 38761\n",
      "Categories: 12\n",
      "\n",
      "per category:\n",
      "Class    GT  Recall  Precision  AP@0.5       F1\n",
      "  MTL 29720  0.5027     0.1479  0.1914 0.228556\n",
      "  CRC   573  0.4181     0.0628  0.1344 0.109198\n",
      "  GWA  4135  0.1734     0.1398  0.0502 0.154798\n",
      "  SWA  3423  0.4078     0.0842  0.0895 0.139580\n",
      "  BND   708  0.5732     0.1957  0.2286 0.291781\n",
      "  SLE    58  0.7931     0.2754  0.6854 0.408834\n",
      "  BRN    47  0.5556     0.2941  0.4434 0.384611\n",
      "  TEE    36  0.7368     0.5185  0.6546 0.608669\n",
      "  CAS    25  0.3200     0.4444  0.2143 0.372077\n",
      "  VAL    23  0.8696     0.6667  0.7032 0.754751\n",
      "  ESP     9  0.7778     0.3043  0.4765 0.437454\n",
      "  FLA     4  0.5000     0.0541  0.2794 0.097636\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "def parse_metrics_table(text: str) -> Dict[str, Tuple[float, float, float]]:\n",
    "    \"\"\"\n",
    "    \n",
    "    format：\n",
    "    ClassID      Recall    Precision    AP@0.5\n",
    "    ---------  --------  -----------  --------\n",
    "    MTL            0.37         0.40      0.22\n",
    "    ...\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # 按行分割\n",
    "    lines = text.strip().split('\\n')\n",
    "    \n",
    "    # \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or '---' in line or 'ClassID' in line:\n",
    "            continue\n",
    "        \n",
    "        # \n",
    "        parts = re.split(r'\\s+', line)\n",
    "        if len(parts) >= 4:\n",
    "            class_id = parts[0]\n",
    "            try:\n",
    "                recall = float(parts[1])\n",
    "                precision = float(parts[2])\n",
    "                ap50 = float(parts[3])\n",
    "                metrics[class_id] = (recall, precision, ap50)\n",
    "            except ValueError:\n",
    "                print(f\"warning: {line}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def calculate_macro_micro_metrics(metrics: Dict[str, Tuple[float, float, float]], \n",
    "                                  gt_counts: Dict[str, int]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # \n",
    "    all_classes = list(gt_counts.keys())\n",
    "    total_gt = sum(gt_counts.values())\n",
    "    \n",
    "    # \n",
    "    recalls = []\n",
    "    precisions = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    total_tp = 0\n",
    "    total_pred = 0\n",
    "    \n",
    "    for class_id in all_classes:\n",
    "        if class_id in metrics:\n",
    "            recall, precision, _ = metrics[class_id]\n",
    "        else:\n",
    "            recall, precision = 0.0, 0.0  # ",
    "        \n",
    "        recalls.append(recall)\n",
    "        precisions.append(precision)\n",
    "        \n",
    "        # ",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0.0\n",
    "        f1_scores.append(f1)\n",
    "        \n",
    "        # ",
    "        tp = recall * gt_counts[class_id]\n",
    "        total_tp += tp\n",
    "        \n",
    "        if precision > 0:\n",
    "            pred = tp / precision\n",
    "        else:\n",
    "            pred = 0\n",
    "        total_pred += pred\n",
    "    \n",
    "    #",
    "    results['Macro-P'] = np.mean(precisions) if precisions else 0\n",
    "    results['Macro-R'] = np.mean(recalls) if recalls else 0\n",
    "    results['Macro-F1'] = np.mean(f1_scores) if f1_scores else 0\n",
    "    \n",
    "    #",
    "    if total_pred > 0:\n",
    "        results['Micro-P'] = total_tp / total_pred\n",
    "    else:\n",
    "        results['Micro-P'] = 0\n",
    "    \n",
    "    if total_gt > 0:\n",
    "        results['Micro-R'] = total_tp / total_gt\n",
    "    else:\n",
    "        results['Micro-R'] = 0\n",
    "    \n",
    "    if results['Micro-P'] + results['Micro-R'] > 0:\n",
    "        results['Micro-F1'] = 2 * results['Micro-P'] * results['Micro-R'] / (results['Micro-P'] + results['Micro-R'])\n",
    "    else:\n",
    "        results['Micro-F1'] = 0\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # ",
    "    input_text = \"\"\"\n",
    "\n",
    "Class      Recall    Precision    AP@0.5\n",
    "-------  --------  -----------  --------\n",
    "MTL        0.5027       0.1479    0.1914\n",
    "TEE        0.7368       0.5185    0.6546\n",
    "BND        0.5732       0.1957    0.2286\n",
    "CRC        0.4181       0.0628    0.1344\n",
    "BRN        0.5556       0.2941    0.4434\n",
    "GWA        0.1734       0.1398    0.0502\n",
    "SWA        0.4078       0.0842    0.0895\n",
    "ESP        0.7778       0.3043    0.4765\n",
    "VAL        0.8696       0.6667    0.7032\n",
    "FLA        0.5000       0.0541    0.2794\n",
    "CAS        0.3200       0.4444    0.2143\n",
    "SLE        0.7931       0.2754    0.6854\"\"\"\n",
    "    \n",
    "    # \n",
    "    gt_counts = {\n",
    "        'MTL': 29720,\n",
    "        'CRC': 573,\n",
    "        'GWA': 4135,\n",
    "        'SWA': 3423,\n",
    "        'BND': 708,\n",
    "        'SLE': 58,\n",
    "        'BRN': 47,\n",
    "        'TEE': 36,\n",
    "        'CAS': 25,\n",
    "        'VAL': 23,\n",
    "        'ESP': 9,\n",
    "        'FLA': 4\n",
    "    }\n",
    "    \n",
    "    # \n",
    "    metrics = parse_metrics_table(input_text)\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f\"{'category':<5} {'Recall':<8} {'Precision':<10} {'AP@0.5':<8}\")\n",
    "    print(\"-\" * 35)\n",
    "    for class_id, (recall, precision, ap50) in metrics.items():\n",
    "        print(f\"{class_id:<5} {recall:<8.4f} {precision:<10.4f} {ap50:<8.4f}\")\n",
    "    print()\n",
    "    \n",
    "    #n",
    "    results = calculate_macro_micro_metrics(metrics, gt_counts)\n",
    "    \n",
    "    # ",
    "    print(\":\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"{'':<15} {'':<10}\")\n",
    "    print(\"-\" * 40)\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key:<15} {value:.4f}\")\n",
    "    \n",
    "    # mAP（）\n",
    "    ap50_values = [ap50 for _, _, ap50 in metrics.values()]\n",
    "    mAP = np.mean(ap50_values) if ap50_values else 0\n",
    "    print(f\"{'mAP@0.5':<15} {mAP:.4f}\")\n",
    "    \n",
    "    # \n",
    "    print(\"\\n:\")\n",
    "    print(f\": {sum(gt_counts.values())}\")\n",
    "    print(f\": {len(gt_counts)}\")\n",
    "    \n",
    "    # \n",
    "    data = []\n",
    "    for class_id in gt_counts.keys():\n",
    "        if class_id in metrics:\n",
    "            recall, precision, ap50 = metrics[class_id]\n",
    "        else:\n",
    "            recall, precision, ap50 = 0, 0, 0\n",
    "        \n",
    "        if precision + recall > 0:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        \n",
    "        data.append({\n",
    "            'Class': class_id,\n",
    "            'GT': gt_counts[class_id],\n",
    "            'Recall': recall,\n",
    "            'Precision': precision,\n",
    "            'AP@0.5': ap50,\n",
    "            'F1': f1\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"\\n:\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a339707-6277-4aba-8b8a-a84c98c1d9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
